# Data_Science_DL

âœ… Deep Learning
    Neurons, Neural Network, Input Layer, Hidden Layer, Output Layer, Weights, Bias, Batch, Epochs, Iterations, Learning Rate, Multi Layer Perceptron, Gradient Descent, Activation Functions, Sigmoid Activation Function, Tanh Activation Function, ReLU Activation Function, Leaky ReLU Activation Function, Softmax Activation Function, Forward Propagation, Backward Propagation, Chain Rule of Derivative, Vanishing Gradient Problem, Exploding Gradient Problem, Optimizers, Adadelta, Adagrad, Adam, RMSProp, SGD, Dropout Layers, Data Augmentation, Flattening, Max Pooling, Kernels, ANN, CNN, RNN, LSTM, GAN, Model Deployment, Deep Learning Projects